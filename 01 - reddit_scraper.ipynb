{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b30bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from scripts import data_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3f54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file containing Reddit credentials stored in dictionary\n",
    "\n",
    "file = open(\"reddit_credentials.txt\", \"r\")\n",
    "contents = file.read()\n",
    "reddit_credentials = ast.literal_eval(contents)\n",
    "file.close()\n",
    "\n",
    "# define reddit credentials using imported dictionary\n",
    "client_id = reddit_credentials[\"client_id\"]\n",
    "client_secret = reddit_credentials[\"client_secret\"]\n",
    "user_agent = reddit_credentials[\"user_agent\"]\n",
    "username = reddit_credentials[\"username\"]\n",
    "password = reddit_credentials[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036cae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.3.0 was released Thursday June 17, 2021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-17\n"
     ]
    }
   ],
   "source": [
    "# Scrape sentiment data\n",
    "\n",
    "# Scraper class takes 9 arguments:\n",
    "# Date to start scraping data\n",
    "# Date to stop scraping data\n",
    "# Subreddit to scrape\n",
    "# Query term (it will look for comments where this term is present in the post and comment)\n",
    "# Also: client_id, client_secret, user_agent, username, and password\n",
    "\n",
    "# Output is a CSV file containing the following columns for each scraped comment:\n",
    "# ID of comment\n",
    "# Date that comment was posted ('%Y-%m-%d')\n",
    "# Body of the comment\n",
    "# Score (upvotes) of comment\n",
    "# Positive sentiment score\n",
    "# Negative sentiment score\n",
    "# Neutral sentiment score\n",
    "# Compound sentiment score\n",
    "# The CSV file will be named as \"subreddit\"+_+\"query\".CSV \n",
    "# and output into same directory as the notebook.\n",
    "\n",
    "# While running, the class prints the current date being collected. \n",
    "# I found this to be useful when debugging issues with the API hanging. \n",
    "\n",
    "# For each date, only the first100 posts containing the query are scraped, \n",
    "# and for each post, only the first 100 comments containing the query are scraped.\n",
    "# This caps the total possible comments at 10,000 per day,\n",
    "# which should be plenty of data to analyze without abusing the API too much.\n",
    "\n",
    "data_scraper.Scraper(\"2021-07-17\",\"2021-07-18\", \"conservative\", \"vaccine\", client_id, \n",
    "       client_secret, user_agent, username, password).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7eb38c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-17\n"
     ]
    }
   ],
   "source": [
    "# Let's collect the same data from the r/politics subreddit\n",
    "data_scraper.Scraper(\"2021-07-17\",\"2021-07-18\", \"politics\", \"vaccine\", client_id, \n",
    "       client_secret, user_agent, username, password).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3047182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-17\n"
     ]
    }
   ],
   "source": [
    "# And also from the r/COVID19 subreddit\n",
    "data_scraper.Scraper(\"2021-07-17\",\"2021-07-18\", \"COVID19\", \"vaccine\", client_id, \n",
    "       client_secret, user_agent, username, password).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebda4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-17\n"
     ]
    }
   ],
   "source": [
    "# And, data from  the r/conspiracy subreddit \n",
    "data_scraper.Scraper(\"2021-7-17\",\"2021-7-18\", \"conspiracy\", \"vaccine\", client_id, \n",
    "       client_secret, user_agent, username, password).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993054ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-17\n"
     ]
    }
   ],
   "source": [
    "# And from r/worldnews \n",
    "data_scraper.Scraper(\"2021-07-17\",\"2021-07-18\", \"worldnews\", \"vaccine\", client_id, \n",
    "       client_secret, user_agent, username, password).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a4fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
